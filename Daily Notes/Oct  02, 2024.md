Setting up the cloud-based transcription method using **Azure Blob Storage** and **Azure Functions** involves a few steps. I will walk you through how to set up this method, including configuration of the Azure services and deploying your code for automated audio transcription using WhisperX. By the end of this setup, you’ll have an automated system where users upload audio files, and the system processes and stores the transcription results.

### **Step-by-Step Setup**

#### **1. Create an Azure Account (if you don’t already have one)**
1. If you don't already have an Azure account, go to [Azure.com](https://azure.microsoft.com/) and sign up.
2. Azure offers a free tier, which provides some free resources that can help you get started with this setup at no cost.

#### **2. Set up Azure Blob Storage**
Azure Blob Storage will hold the audio files and the transcription results.

1. **Create a Storage Account**:
   - In the Azure Portal, search for "Storage Accounts" and click "Create".
   - Fill in the required fields (Subscription, Resource Group, Storage Account Name, etc.).
   - Choose **Standard** as the performance tier and **BlobStorage** for the account kind.
   - Click "Review + Create" to create the account.

2. **Create a Container for Audio Files**:
   - In the storage account, go to "Containers".
   - Click "Create Container" and name it something like `audio`.
   - Set the public access level to "Private" (as this is for internal processing).

3. **Create a Container for Transcriptions**:
   - Similarly, create another container named `transcriptions` where the transcription results will be stored.
   - Keep the access level private as well.

#### **3. Install Azure Functions Core Tools Locally**
To develop and test your Azure Functions locally before deploying them to the cloud, you need to install Azure Functions Core Tools.

1. **Install Azure Functions Core Tools**:
   - Download and install the latest version of [Azure Functions Core Tools](https://docs.microsoft.com/en-us/azure/azure-functions/functions-run-local?tabs=windows%2Ccsharp%2Cbash).

2. **Install Python**:
   - Ensure that **Python 3.x** is installed on your machine (if not, download it from the [official website](https://www.python.org/downloads/)).
   - You will need Python for the WhisperX transcription and Azure Function development.

3. **Install the Azure Storage SDK for Python**:
   - In the terminal, run:
     ```bash
     pip install azure-functions azure-storage-blob whisperx torch
     ```

#### **4. Create an Azure Function**
Next, you will create the Azure Function that will process the audio files and trigger transcription automatically.

1. **Create a Local Azure Function Project**:
   - Open a terminal and navigate to the folder where you want to create the project.
   - Initialize an Azure Functions project:
     ```bash
     func init order_transcription_app --python
     ```
   - Navigate to the project folder:
     ```bash
     cd myTranscriptionApp
     ```
   - Create a Blob trigger function to process the uploaded audio:
     ```bash
     func new --name ProcessAudio --template "Blob Trigger"
     ```

2. **Configure the Blob Trigger**:
   - In the newly created function, open `function.json` and configure the **Blob Trigger** path:
     ```json
     {
       "bindings": [
         {
           "name": "myblob",
           "type": "blobTrigger",
           "direction": "in",
           "path": "audio/{name}",
           "connection": "AzureWebJobsStorage"
         }
       ]
     }
     ```
   - This specifies that the function will be triggered when any file is uploaded to the **audio** container.

3. **Install WhisperX and Other Dependencies**:
   - Inside your project folder, create a `requirements.txt` file to list the Python packages that the function will need:
     ```txt
     azure-functions
     azure-storage-blob
     whisperx
     torch
     requests
     pydub
     ```
   - You can install the dependencies locally with:
     ```bash
     pip install -r requirements.txt
     ```

#### **5. Write the Azure Function Code**
Now, modify the function to handle audio processing and transcription.

1. **Modify `__init__.py` in the Azure Function**:
   Here’s an example of how the Azure Function might look:

```python
import logging
import os
import whisperx
from azure.storage.blob import BlobServiceClient
import azure.functions as func
import io

def main(myblob: func.InputStream):
    logging.info(f"Processing audio file: {myblob.name}")
    
    # Load the WhisperX model
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = whisperx.load_model("large", device=device)

    # Read audio file from Blob
    audio_data = myblob.read()  # Get audio content
    audio_path = "/tmp/temp_audio.mp3"  # Temporary file path

    with open(audio_path, "wb") as f:
        f.write(audio_data)

    # Perform transcription using WhisperX
    result = model.transcribe(audio_path)
    aligned_result = whisperx.align(result["segments"], model, device)

    # Format transcription into HTML
    html_content = generate_html_from_transcription(aligned_result)

    # Upload the HTML transcription to Blob Storage
    blob_service_client = BlobServiceClient.from_connection_string(os.getenv("AzureWebJobsStorage"))
    output_blob_client = blob_service_client.get_blob_client(container="transcriptions", blob=f"{myblob.name}.html")
    output_blob_client.upload_blob(html_content, overwrite=True)

def generate_html_from_transcription(aligned_result):
    html_content = "<html><body><h1>Transcription Result</h1><ul>"
    for segment in aligned_result["segments"]:
        speaker = segment["speaker"]
        text = segment["text"].strip()
        html_content += f"<li><strong>Speaker {speaker}:</strong> {text}</li>"
    html_content += "</ul></body></html>"
    return html_content
```

2. **Environment Variables**:
   - Ensure the connection string for Azure Blob Storage (`AzureWebJobsStorage`) is set up in the environment settings for the function.
   - You can find the connection string in your Storage Account under **Access keys**.

#### **6. Deploy the Azure Function**
Now that your function is written and tested locally, it's time to deploy it to Azure.

1. **Login to Azure**:
   - If you’re using Azure CLI, run:
     ```bash
     az login
     ```

2. **Deploy the Azure Function**:
   - To deploy your function, run:
     ```bash
     func azure functionapp publish <YourFunctionAppName>
     ```

   This will deploy your function to Azure and make it live.

#### **7. Test the End-to-End Process**
1. **Upload Audio Files**:
   - Once the function is deployed, upload an audio file to the **audio** container in Azure Blob Storage (you can do this manually via the Azure Portal or automate it via APIs).
   
2. **Check Transcription Results**:
   - The function will automatically trigger, process the audio, and store the transcription results in the **transcriptions** container.

3. **Receive Notifications** (optional):
   - You can extend the functionality with Azure Logic Apps or Azure Functions to send out an email or notification when the transcription is ready.

#### **8. Optional: Set Up a Simple Frontend**
If you want to provide a user-friendly web interface for users to upload audio and view the transcription results, consider using **Azure Static Web Apps** or a simple **HTML/JavaScript page**.

1. **Azure Static Web Apps**:
   - You can create a simple website where users can upload audio files directly to Blob Storage, and the backend (Azure Functions) will handle the transcription.
   - Users can also access a link to the transcription results.

#### **Summary of Steps**:
1. **Create Azure Blob Storage**: Store audio files and transcriptions.
2. **Install Azure Functions Core Tools**: Set up local development.
3. **Create a Blob-triggered Azure Function**: Automatically transcribe uploaded audio.
4. **Install WhisperX**: Transcribe audio files using WhisperX in the Azure cloud.
5. **Deploy the Function to Azure**: Publish the function to Azure.
6. **Test the Full Workflow**: Upload audio files and check the transcription results.
7. (Optional) **Add Notifications or Frontend**: Notify users or provide a web interface.

By following these steps, you’ll have a fully functional cloud-based audio transcription service that leverages Azure resources to handle audio processing and transcription with WhisperX.


------------------------------
--------------------------

To configure an **Azure Function App** to handle the **WhisperX large model**, you will need to consider a few important factors such as managing the model size, resource allocation (especially memory and compute resources), and deployment settings for efficient handling. The **WhisperX large model** requires sufficient memory, CPU, or GPU, which can be handled by customizing your Function App settings and choosing the right hosting plan.

Here’s a step-by-step guide to configuring an Azure Function App to handle the WhisperX large model:

### **1. Choose the Appropriate Hosting Plan**

The WhisperX large model requires more resources, so the default **Consumption Plan** might not be enough due to limits on memory (1.5GB) and CPU. You should consider using one of the following hosting plans:

- **Premium Plan**: Offers more memory (up to 14GB) and CPU power, plus the ability to attach a GPU-enabled VM if needed.
- **Dedicated (App Service) Plan**: Provides full control over memory and CPU, ideal for handling larger models.

Here’s how to choose the appropriate plan during the creation of your Function App:

1. In the **Azure Portal**, navigate to **Create a resource** → **Function App**.
2. During the setup, under the **Hosting** tab, choose either:
   - **Premium Plan (Elastic Premium)**: Good for performance and autoscaling.
   - **App Service Plan**: Allows you to choose larger VM sizes (e.g., D-series, E-series VMs) with higher memory.
3. Select a region close to your location or where the bulk of your users are located to reduce latency.

> **Note**: If you need GPU support, you would need to deploy to a **Virtual Machine** with GPU in an **App Service Plan** or use **Azure Machine Learning** services, though it might increase costs.

### **2. Increase Resource Allocation (Memory & CPU)**

For WhisperX large model inference, a minimum of 4GB RAM and 2-4 CPU cores is recommended. You should configure your Function App to have sufficient resources.

#### Premium Plan Setup:

- **Memory (RAM)**: Allocate 4GB or more.
- **CPU Cores**: The Premium plan allows you to configure more cores (2 to 4).
  
   Example VM SKU: **EP1** (3.5 GB RAM) or **EP2** (7 GB RAM).

#### Dedicated Plan Setup:

- **VM Type**: Choose larger VMs with more memory (e.g., D-series, E-series VMs).
   Example VM SKUs:
   - **D2_v3**: 2 vCPU, 8GB RAM
   - **E4s_v3**: 4 vCPU, 32GB RAM (for larger models and heavier processing)
   
### **3. Increase the Maximum Timeout for Azure Functions**

Processing larger audio files with the WhisperX large model can take longer. By default, Azure Functions have a timeout of 5 minutes on the **Consumption Plan**, but this can be increased with the Premium or Dedicated plans.

1. In the Azure Portal, navigate to your Function App.
2. Under **Settings** > **Configuration** > **General settings**, you can configure:
   - **Function timeout**: Set this to a higher value, such as 15 minutes or more, depending on your expected workload.

### **4. Optimize the Function for WhisperX Model Inference**

Here’s how to modify your Azure Function App to handle the large WhisperX model efficiently:

#### A. **Add WhisperX Model and Dependencies in Function App**

You need to ensure that the **WhisperX** model and its dependencies are available in the Azure Function environment.

1. **Create a `requirements.txt` file** that includes all necessary dependencies:
   
   ```txt
   azure-functions
   whisperx
   torch
   azure-storage-blob
   pydub
   requests
   ```

2. **Ensure GPU support**: If you are using a VM with GPU or are running on a premium plan that supports GPU (especially for heavy workloads), you need to install the GPU version of PyTorch.

   For **CUDA-enabled** GPU support, modify your `requirements.txt`:
   ```txt
   torch==<compatible-gpu-version>+cu117  # Adjust based on CUDA version of your VM
   whisperx
   ```

   For CPU-only instances:
   ```txt
   torch==<cpu-version>  # For example, 'torch==2.0.0+cpu'
   whisperx
   ```

   Azure supports both **CPU** and **GPU** depending on the pricing tier you select.

#### B. **Load the Large Model Efficiently**

In your Azure Function code (`__init__.py`), ensure the model is loaded efficiently and cached to prevent loading on every request, which can slow down performance.

Here’s an example of how to load the **WhisperX large model**:

```python
import whisperx
import torch

# Load the model globally to prevent re-loading for each invocation
device = "cuda" if torch.cuda.is_available() else "cpu"
model = whisperx.load_model("large", device=device)  # WhisperX large model

def main(myblob: func.InputStream):
    # Your code here for handling Blob processing and audio transcription
    logging.info(f"Processing audio file: {myblob.name}")
    
    # Transcribe the audio using the loaded model
    audio_data = myblob.read()
    audio_path = "/tmp/temp_audio.mp3"
    
    with open(audio_path, "wb") as f:
        f.write(audio_data)

    # Perform transcription
    result = model.transcribe(audio_path)
    aligned_result = whisperx.align(result["segments"], model, device)

    # Your remaining logic for uploading results back to Blob Storage...
```

By loading the model once globally (outside the `main()` function), you ensure that the WhisperX model is only loaded once per instance, not for every single function invocation, making it more efficient.

#### C. **Use Local Disk Storage for Large Audio Files**

Azure Functions use temporary storage (`/tmp`) for local processing. Ensure your function code writes large audio files to this directory before processing with WhisperX.

Example:
```python
audio_path = "/tmp/temp_audio.mp3"  # Temporary file path
with open(audio_path, "wb") as f:
    f.write(audio_data)
```

This avoids loading large audio files into memory, preventing memory overflows in larger models.

### **5. Deploy the Azure Function App**

Once your function is configured for the **WhisperX large model**, you need to deploy it to Azure.

1. **Publish your function**:
   After testing locally with `func start`, deploy to Azure using the Azure CLI or Visual Studio Code:
   
   ```bash
   func azure functionapp publish <YourFunctionAppName>
   ```

2. **Monitor the Function**:
   In the Azure portal, use **App Insights** or **Logs** to monitor the performance, resource usage (memory and CPU), and any failures.

### **6. Scale the Function App (Optional)**

For large workloads (many audio files or large audio files), you might need to scale your Function App horizontally. Azure allows you to configure auto-scaling based on the load.

1. **Set Auto-Scaling Rules**:
   - In the **Azure Portal**, navigate to your Function App.
   - Under the **Scale out (App Service Plan)** section, configure auto-scaling based on CPU or memory usage to spin up additional instances of the function during high loads.

   For example, you can set rules to add more instances when CPU usage goes above 70%, or you can configure a **fixed instance count** if you know the workload in advance.

### **7. Ensure Storage Performance (Optional)**

Ensure your **Azure Blob Storage** account is using the right **performance tier** for faster I/O when reading large audio files:

- Use **Hot Access Tier** for frequently accessed audio files.
- Use **Standard Performance** for general-purpose workloads, or **Premium Performance** if you expect high I/O (reading and writing large files frequently).

### **Final Setup Overview**

1. **Azure Premium Plan or Dedicated Plan**:
   - Sufficient resources to handle WhisperX large model (4+ GB RAM, multiple CPUs).
   
2. **Timeout and Scaling**:
   - Increase timeout limits and configure auto-scaling if required.

3. **Optimized WhisperX Usage**:
   - Cache the model in memory to avoid reloading on every function execution.

4. **Efficient File Handling**:
   - Use temporary storage (`/tmp`) for large audio files and make sure you manage memory carefully.

### **Conclusion**

With the above steps, you can configure your **Azure Function App** to handle WhisperX large model transcriptions efficiently. By choosing the right hosting plan, allocating sufficient memory, and optimizing model loading, your function will be able to transcribe large audio files in a scalable and cost-effective way.


-------------------------
------------------------------------------------------------------

Yes, you will need to install **FFmpeg** if you're working with **WhisperX** (and Whisper in general) because it relies on FFmpeg to handle audio file conversions, especially if you're dealing with formats like MP3, WAV, or other non-raw audio formats.

**FFmpeg** is a popular multimedia framework used to decode, encode, and convert various audio and video formats, which is essential when processing audio for transcription with WhisperX.

### **Why You Need FFmpeg for WhisperX**:
- **WhisperX** internally uses PyDub or similar libraries that rely on FFmpeg to handle audio file formats (e.g., converting MP3 to a format Whisper can process, such as WAV).
- Without FFmpeg, the transcription process may fail when trying to load or convert certain audio formats.

### **Installing FFmpeg on Azure Functions**

Unfortunately, **Azure Functions** doesn't have FFmpeg pre-installed by default. However, you can work around this by including FFmpeg as part of your deployment.

There are two ways to do this:

### **Option 1: Install FFmpeg Locally on the Function App**

1. **Download FFmpeg Executables**:
   - Download a static build of FFmpeg for Linux (since Azure Functions typically run on a Linux-based platform).
     - You can find static builds of FFmpeg on this site: [FFmpeg Static Builds](https://johnvansickle.com/ffmpeg/).
     - Download a precompiled version that works with your Azure Function (likely the 64-bit version for Linux).

2. **Include FFmpeg in Your Azure Function**:
   - Unzip the downloaded FFmpeg and place the binary files (such as `ffmpeg`, `ffplay`, `ffprobe`) inside your **Azure Function project** folder, ensuring they are placed in a directory where the function can access them.

3. **Set Executable Permissions**:
   - Before using FFmpeg in Azure Functions, ensure the FFmpeg files have the correct permissions by using the `chmod` command to make them executable in your Azure environment.
   - This might look like:
     ```bash
     chmod +x ffmpeg
     chmod +x ffprobe
     ```

4. **Add FFmpeg to PATH (Optional)**:
   - If needed, update the `PATH` environment variable in your Azure Function so it knows where to find FFmpeg. In your Python code, you can reference the FFmpeg binaries directly like so:

     ```python
     import os
     os.environ["PATH"] += os.pathsep + os.path.abspath("/path/to/ffmpeg")
     ```

5. **Use FFmpeg in Your Function**:
   - WhisperX and PyDub will automatically use FFmpeg as long as it's accessible. You don't need to manually invoke it. When WhisperX or PyDub needs to convert or process an audio file, FFmpeg will handle that in the background.

### **Option 2: Use a Custom Docker Container (More Complex)**

If you need more control over the environment or you want to pre-install FFmpeg, you can create a custom **Docker container** for your Azure Function.

1. **Create a Dockerfile**:
   - The Dockerfile would include the installation of **FFmpeg**, along with any other dependencies (e.g., WhisperX, Torch, etc.).
   
   Here’s a sample Dockerfile that installs FFmpeg:

   ```dockerfile
   FROM mcr.microsoft.com/azure-functions/python:4-python3.9

   # Install ffmpeg
   RUN apt-get update && \
       apt-get install -y ffmpeg

   # Install Python dependencies
   COPY requirements.txt /
   RUN pip install -r /requirements.txt

   # Copy function app code
   COPY . /home/site/wwwroot
   ```

2. **Build and Push the Docker Image**:
   - Build the image locally:
     ```bash
     docker build -t yourdockerhubaccount/yourimage:tag .
     ```
   - Push the image to a Docker registry (like Docker Hub or Azure Container Registry):
     ```bash
     docker push yourdockerhubaccount/yourimage:tag
     ```

3. **Deploy the Container to Azure Functions**:
   - You can configure your Azure Function App to use this Docker container by updating the **Container Settings** in the Azure Portal to point to your image.

### **Conclusion**:

- **Yes, FFmpeg is required** for WhisperX to handle audio file conversion and processing.
- The easiest way to include FFmpeg in an Azure Function is by **downloading a precompiled static build** of FFmpeg and adding it to your function’s directory or using a custom **Docker container** that includes FFmpeg.

This will ensure that WhisperX can process and transcribe audio files in various formats like MP3 and WAV without running into file format issues.

-----------------
---------------------

Based on the directory structure shown in the image of the `ORDER_TRANSCRIPTION_APP` project, here’s an updated step-by-step guide for setting up and running this transcription project.

### **Project Layout:**
```
ORDER_TRANSCRIPTION_APP/
│
├── .venv/               # Virtual environment directory for Python packages (not visible in the project)
├── templates/
│   └── index.html       # HTML template for rendering the web page (UI for file upload)
├── transcriptions/      # Directory to store transcribed files (output)
├── uploads/             # Directory to store uploaded audio files (input)
├── .gitignore           # Specifies files and directories to be ignored by Git
├── app.py               # Main application code (Flask or other web framework)
├── README.md            # Project documentation and instructions
└── requirements.txt     # Python dependencies file
```

### **Step-by-Step Setup Guide:**

#### **Step 1: Install Python and Set Up a Virtual Environment**

1. **Install Python**:
   Ensure you have Python 3.8+ installed on your machine. You can download it from [python.org](https://www.python.org/downloads/).

2. **Create a Virtual Environment**:
   - Navigate to your project directory (`ORDER_TRANSCRIPTION_APP`) in your terminal or command prompt:
     ```bash
     cd ORDER_TRANSCRIPTION_APP
     ```
   - Create a virtual environment inside the project folder:
     ```bash
     python -m venv .venv
     ```
   - Activate the virtual environment:
     - **Windows**:
       ```bash
       .venv\Scripts\activate
       ```
     - **macOS/Linux**:
       ```bash
       source .venv/bin/activate
       ```

#### **Step 2: Install Dependencies**

1. **Install Required Libraries**:
   With the virtual environment activated, install all the necessary dependencies listed in the `requirements.txt` file:
   ```bash
   pip install -r requirements.txt
   ```

   The `requirements.txt` likely includes dependencies for:
   - **Flask** (if you’re using Flask for the web app)
   - **WhisperX** or similar transcription models
   - **FFmpeg support** for audio processing
   - **Other audio processing libraries** like PyDub

   Example of what `requirements.txt` might contain:
   ```txt
   flask
   whisperx
   torch
   pydub
   ffmpeg-python
   ```

#### **Step 3: Configure FFmpeg (If Needed)**

If the application needs to process audio formats like MP3, you'll need **FFmpeg** installed on your machine. This is usually used alongside libraries like PyDub for audio manipulation.

- Download and install **FFmpeg** from [here](https://ffmpeg.org/download.html).
- Make sure FFmpeg is added to your system's PATH so it can be used by the Python application.

#### **Step 4: Review `app.py` (Main Application Code)**

- The `app.py` file is likely the main entry point for running the web app. Typically, this file will:
  - Set up a Flask server to handle file uploads and process them for transcription.
  - Define routes like:
    - **File Upload Route** (for uploading audio files to the `uploads/` directory).
    - **Transcription Process** (processing the uploaded file using WhisperX or another transcription model).
    - **Displaying Results** (storing the output in the `transcriptions/` directory and showing the results on a web page).

   Here’s an example of how `app.py` might look using Flask:

   ```python
   from flask import Flask, render_template, request, redirect, url_for
   import os
   import whisperx
   from pydub import AudioSegment

   app = Flask(__name__)

   UPLOAD_FOLDER = 'uploads/'
   TRANSCRIPT_FOLDER = 'transcriptions/'

   app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER

   # Ensure the upload and transcription directories exist
   os.makedirs(UPLOAD_FOLDER, exist_ok=True)
   os.makedirs(TRANSCRIPT_FOLDER, exist_ok=True)

   @app.route('/')
   def index():
       return render_template('index.html')

   @app.route('/upload', methods=['POST'])
   def upload_file():
       if 'file' not in request.files:
           return "No file part"
       file = request.files['file']
       if file.filename == '':
           return "No selected file"

       # Save the file to the uploads directory
       filepath = os.path.join(app.config['UPLOAD_FOLDER'], file.filename)
       file.save(filepath)

       # Perform transcription using WhisperX
       transcription_result = transcribe_audio(filepath)

       # Save transcription result to the transcription folder
       transcription_path = os.path.join(TRANSCRIPT_FOLDER, f"{file.filename}.txt")
       with open(transcription_path, "w") as f:
           f.write(transcription_result)

       return redirect(url_for('index'))

   def transcribe_audio(filepath):
       # Load the WhisperX model
       device = "cuda" if torch.cuda.is_available() else "cpu"
       model = whisperx.load_model("large", device=device)

       # Perform transcription
       result = model.transcribe(filepath)
       return result['text']

   if __name__ == "__main__":
       app.run(debug=True)
   ```

   This app:
   - Renders an HTML form (from `templates/index.html`) to allow file uploads.
   - Processes uploaded files, runs transcription using WhisperX, and saves the results in the `transcriptions/` directory.

#### **Step 5: Set Up Templates for the Web Interface**

1. **index.html**:
   - The `templates/index.html` file is the HTML front-end for the web app. It might look something like this:

   ```html
   <!DOCTYPE html>
   <html lang="en">
   <head>
       <meta charset="UTF-8">
       <meta name="viewport" content="width=device-width, initial-scale=1.0">
       <title>Order Transcription App</title>
   </head>
   <body>
       <h1>Upload Audio File for Transcription</h1>
       <form action="/upload" method="post" enctype="multipart/form-data">
           <input type="file" name="file">
           <input type="submit" value="Upload">
       </form>
   </body>
   </html>
   ```

   This HTML form allows users to upload an audio file, which will be sent to the Flask app for processing.

#### **Step 6: Running the Application Locally**

1. **Run the Flask Application**:
   - Start the Flask server by running the `app.py` script:
     ```bash
     python app.py
     ```

   - By default, the Flask application will run locally at `http://127.0.0.1:5000/`.

2. **Access the Application**:
   - Open a web browser and go to `http://127.0.0.1:5000/` to see the file upload interface.
   - Upload an audio file, and the app will process it and store the transcription result in the `transcriptions/` directory.

#### **Step 7: View Transcriptions**

- After successfully uploading an audio file, the transcription result will be saved in the `transcriptions/` folder.
- You can access the transcription results manually by checking the contents of this folder, or you can modify the Flask app to display the transcription on a results page.

#### **Step 8: Git Setup (Optional)**

- The project already includes a `.gitignore` file. This file ensures that unnecessary files (e.g., Python virtual environment and temporary files) are not included in your Git repository.
  
  Example of a `.gitignore` file:
  ```gitignore
  .venv/
  __pycache__/
  *.pyc
  uploads/
  transcriptions/
  ```

### **Additional Notes**:

- **WhisperX**: Ensure that you’ve properly installed WhisperX and Torch in the `requirements.txt` file, and ensure that the system has access to the right model versions.
- **FFmpeg**: If your app processes MP3 or other audio formats, ensure that FFmpeg is installed and available in the environment.
- **Error Handling**: Consider adding more robust error handling in the `app.py` file to handle edge cases like invalid file types or large file uploads.

---

### **Summary of Key Steps**:
1. Set up a virtual environment and install dependencies.
2. Configure the Flask application to handle file uploads and transcription.
3. Install FFmpeg for audio processing if needed.
4. Run the Flask application and upload audio files for transcription.
5. Store transcription results in the `transcriptions/` directory.
6. (Optional) Customize the frontend and error handling as needed.

This setup will give you a fully functional transcription application running locally, capable of receiving audio files, processing them with WhisperX, and saving the transcription results.


---------
--------

To modify this code to use WhisperX instead of Whisper, we need to adjust the audio processing and transcription parts. WhisperX is a variant of Whisper that supports word-level timestamps and uses VAD (Voice Activity Detection) for better segmentation. I'll outline the necessary changes and rewrite the relevant parts:

### Key Changes:
1. Replace `whisper` with `whisperx`.
2. Use WhisperX-specific model loading, transcription methods, and timestamp extraction.
3. Adapt the transcribe function to handle WhisperX output, including word-level timestamps.

Here's the rewritten code using `whisperx`:

```python
import io
import os
import warnings
import time
import datetime
import requests
import torch
import whisperx  # Use whisperx instead of whisper
import numpy as np
from pydub import AudioSegment
from pydub.silence import detect_silence
from colorama import Fore, Style, init
import re

# Function to fetch data from the API using the provided parameters
def fetch_data(api_url, params):
    """Fetch data from API using the specified parameters."""
    try:
        # Debug: print the actual request URL with parameters
        print(f"Request URL: {requests.Request('GET', api_url, params=params).prepare().url}")

        response = requests.get(api_url, params=params)
        response.raise_for_status()  # Raise an error for bad status codes

        # Debug: print the raw response content
        print(f"Response status code: {response.status_code}")
        print(f"Response content: {response.text}")

        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"Error fetching data from API: {e}")
        return []

# Function to download audio files
def download_audio(url):
    """Download audio file and return it as a BytesIO object."""
    if not url:
        print("No audio URL provided.")
        return None
    response = requests.get(url)
    if response.status_code != 200:
        print(f"Failed to stream audio from {url}. Status: {response.status_code}")
        return None
    return io.BytesIO(response.content)

# Function to calculate call duration in seconds
def calculate_duration(call_in_time, call_out_time):
    """Calculate call duration in seconds given the call in and out times in AM/PM format."""
    call_in = datetime.datetime.strptime(call_in_time, "%I:%M:%S %p")
    call_out = datetime.datetime.strptime(call_out_time, "%I:%M:%S %p")
    duration = call_out - call_in
    return duration

# Function to convert audio to numpy array
def audio_to_numpy(audio):
    """Convert pydub AudioSegment to numpy array."""
    samples = audio.get_array_of_samples()
    return np.array(samples).astype(np.float32) / 32768.0  # Normalize to [-1.0, 1.0]

# Function to sanitize filenames
def sanitize_filename(name):
    """Sanitize the store name to create a valid file name."""
    return re.sub(r'[^\w\s-]', '', name).replace(' ', '_')

# Function to create the transcription folder on the desktop
def create_transcription_folder():
    """Create the 'Transcription_Results' folder on the desktop if it doesn't exist."""
    desktop_path = os.path.join(os.path.expanduser("~"), "Desktop")
    folder_path = os.path.join(desktop_path, "Transcription_Results")

    if not os.path.exists(folder_path):
        os.makedirs(folder_path)
        print(f"Created folder: {folder_path}")
    else:
        print(f"Folder already exists: {folder_path}")

    return folder_path

# Function to process and transcribe each call using WhisperX
def process_call(order_id):
    """Process and transcribe call for the given order ID."""

    # Load WhisperX model here, so it is ready for each order ID
    try:
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model = whisperx.load_model("base", device)  # WhisperX model
        print("WhisperX model loaded.")
    except Exception as e:
        print(f"Error loading WhisperX model: {e}")
        return

    # Define the query parameters to search by `orderId` only
    params = {
        "page": 1,       # Page number
        "limit": 1,      # Limit number of results (e.g., 1)
        "orderId": order_id  # The Order ID to search for
    }

    # Define the API endpoint
    api_url = "https://telephony.ordrai.com/calls"

    # Start timer for measuring script execution time
    start_time = time.time()

    # Fetch the data
    response = fetch_data(api_url, params)

    # Ensure the response is a dictionary or list and check for 'data'
    if isinstance(response, dict) and 'data' in response:
        call_data = response['data']
    elif isinstance(response, list):
        call_data = response
    else:
        print(f"Unexpected API response format: {type(response)}")
        call_data = []

    if not call_data:
        print(f"No data found for Order ID: {order_id}")
        return None

    # Process the retrieved call data
    for call in call_data:
        print(f"Processing data for call ID: {call.get('callSid')}")

        store_date = call.get('storeDate')
        call_time = call.get('callTimeAMPM')
        store_name = call.get('storeName')
        agent_name = call.get('agentName', 'Unknown')
        watched_by = call.get('watchedBy', 'No One')
        customer_phone = call.get('custId')
        order_id = call.get('orderId', 'N/A')  # Handle empty orderId
        call_type = call.get('orderType')
        queue_type = call.get('queueName')
        call_status = call.get('callStatus')
        call_in_time = call.get('callInTimeAMPM')
        call_out_time = call.get('callOutTimeAMPM')
        call_duration = calculate_duration(call_in_time, call_out_time) if call_in_time and call_out_time else 'N/A'

        # Sanitize store name for the filename
        sanitized_store_name = sanitize_filename(store_name)

        # Create the transcription folder on the desktop if it doesn't exist
        transcription_folder = create_transcription_folder()

        # Generate the full path for the HTML file
        output_file_path = os.path.join(transcription_folder, f"{sanitized_store_name}.html")

        # HTML content initialization for the new transcription
        new_html_content = f"""
        <h2>New Transcription Results for {store_name}</h2>
        <h3><strong>Audio File for Store {store_name} ({order_id})</strong></h3>
        <p>
            <strong>Date:</strong> {store_date}<br>
            <strong>Time:</strong> {call_time}<br>
            <strong>Store Name:</strong> {store_name}<br>
            <strong>Agent Name:</strong> {agent_name}<br>
            <strong>Watched By:</strong> {watched_by}<br>
            <strong>Customer Phone:</strong> {customer_phone}<br>
            <strong>Order ID:</strong> {order_id}<br>
            <strong>Call Type:</strong> {call_type}<br>
            <strong>Queue Type:</strong> {queue_type}<br>
            <strong>Call Status:</strong> {call_status}<br>
            <strong>Call in Time:</strong> {call_in_time}<br>
            <strong>Call out Time:</strong> {call_out_time}<br>
            <strong>Call Duration:</strong> {call_duration}
        </p>
        <ol>
        """

        # Download audio and process transcription and silence detection
        recording_url = call.get('recordingURL')
        if not recording_url:
            print(f"No audio URL for call with call ID: {call.get('callSid')}. Skipping.")
            continue

        audio_content = download_audio(recording_url)
        if audio_content:
            try:
                # Load the audio using PyDub directly from the BytesIO object
                audio = AudioSegment.from_mp3(audio_content)

                # Analyze the average volume of the audio
                average_dbfs = audio.dBFS

                # Detect silence ranges in the audio file
                silence_thresh = average_dbfs - 14
                min_silence_len = 1000
                silence_ranges = detect_silence(audio, min_silence_len=min_silence_len, silence_thresh=silence_thresh)
                silence_ranges_seconds = [(start / 1000, end / 1000, (end - start) / 1000) for start, end in silence_ranges]

                # Convert audio to numpy array for WhisperX
                audio_np = audio_to_numpy(audio)

                # Transcribe the audio using WhisperX with CUDA support and get word-level timestamps
                result = model.transcribe(audio_np)

                # Align words to get word-level timestamps
                alignments = whisperx.align(result["text"], result["segments"], model, device)

                # Combine text and timestamps for the output
                for word_info in alignments['word_segments']:
                    word = word_info['word']
                    start_time = word_info['start']
                    end_time = word_info['end']
                    new_html_content += f"<li>{word} (start: {start_time}, end: {end_time})</li>\n"
                new_html_content += "</ol>\n"

            except Exception as e:
                print(f"Error processing

 audio for call with ID {call.get('callSid')}: {str(e)}")
                continue

        # Calculate the duration of the script
        end_time = time.time()
        duration_minutes = (end_time - start_time) / 60

        # Add the duration to the HTML content
        new_html_content += f"<h2><strong>Script Execution Time:</strong> {duration_minutes:.2f} minutes</h2>\n"

        # Check if the file exists
        if os.path.exists(output_file_path):
            # If the file exists, append new content to the bottom
            with open(output_file_path, "a", encoding="utf-8") as f:
                f.write(new_html_content)
            print(f"New transcription appended to {output_file_path}.")
        else:
            # If the file doesn't exist, create a new file with HTML structure
            full_html_content = f"""
            <!DOCTYPE html>
            <html lang="en">
            <head>
                <meta charset="UTF-8">
                <title>Transcription Results for {store_name}</title>
                <style>
                    .long-silence {{
                        color: red;
                        font-weight: bold;
                    }}
                    .medium-silence {{
                        color: darkorange;
                        font-weight: bold;
                    }}
                    .short-silence {{
                        color: green;
                        font-weight: bold;
                    }}
                </style>
            </head>
            <body>
            """ + new_html_content + """
            </body>
            </html>
            """
            with open(output_file_path, "w", encoding="utf-8") as f:
                f.write(full_html_content)
            print(f"Transcription results have been saved to {output_file_path}.")

### Main Loop to Process Multiple `orderId`s
while True:
    order_ids = input("Enter the Order IDs separated by commas (or type 'exit' to stop): ")

    if order_ids.lower() == "exit":
        print("Exiting the script.")
        break

    # Split the input by commas and strip any extra whitespace
    order_ids_list = [order_id.strip() for order_id in order_ids.split(",")]

    # Loop through each Order ID and process it individually
    for order_id in order_ids_list:
        process_call(order_id)
```

### Changes Summary:
- **WhisperX Model**: Used `whisperx.load_model` to load the WhisperX model.
- **Transcription**: Used WhisperX’s `transcribe` method for audio and `whisperx.align` for word-level timestamps.
- **Output**: The transcription output now includes word-level timestamps in the HTML file.

This should enable you to process and transcribe calls using WhisperX with word-level timestamps. Let me know if you need further adjustments!