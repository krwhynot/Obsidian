Here’s a textual description of the layout and flow of the cloud-based transcription project using **Azure Blob Storage**, **Azure Functions**, and **Azure Speech Service**. You can visualize the project in the following steps:

### Diagram Layout
- **User/Client** (Step 1)
    - Initiates by uploading an audio file (e.g., `sample-audio.mp3`).
  
- **Azure Blob Storage** (Step 2)
    - The uploaded audio file is stored in a designated Blob Storage container (e.g., `audio-files`).

- **Blob Trigger in Azure Function** (Step 3)
    - Azure Function is set up with a Blob Storage trigger. When a new audio file is detected in the Blob Storage, the trigger activates the Azure Function.

- **Azure Function (Transcription Logic)** (Step 4)
    - The function reads the audio file from the Blob Storage.
    - The function initializes the **Azure Speech SDK** with the necessary credentials.
    - The audio file is passed to the Azure Speech SDK for transcription.
  
- **Azure Speech Service** (Step 5)
    - Azure Speech Service transcribes the audio content.
    - The transcription result (text) is returned back to the Azure Function.
  
- **Process and Store Transcription** (Step 6)
    - The Azure Function processes the transcription result (e.g., store in Blob Storage, database, or log).
    - Optionally, it could trigger additional workflows like notifications or save the transcription result to an output Blob container.

### Data Flow:
1. **User/Client Uploads Audio**: The user uploads an audio file via an interface or an API call.
2. **Audio is Stored in Blob Storage**: The file is stored in a specific container in Azure Blob Storage.
3. **Azure Function Triggered**: Blob Storage trigger activates an Azure Function when a new file is uploaded.
4. **Function Processes Audio**: The function reads the audio file and uses Azure Speech SDK to transcribe the audio.
5. **Speech Service Transcription**: The audio is sent to the Azure Speech Service for transcription. The service processes the audio and returns the text.
6. **Store Transcription**: The Azure Function stores or processes the transcription result, e.g., saving to a database, writing to another blob, or outputting to logs.

### Diagram (In Textual Form):
```
+-------------------+         +-------------------+         +-------------------------+
|                   |         |                   |         |                         |
|    User/Client    +--------->  Azure Blob        +--------->  Azure Function         |
|                   | Upload  |  Storage           | Trigger |  (Processes Audio)      |
|                   | Audio   |  (audio-files)     |         |                         |
+-------------------+         +-------------------+         +-------------------------+
                                                             |
                                                             |
                                                             v
                                                +-------------------------+
                                                |                         |
                                                |  Azure Speech Service    |
                                                |  (Transcribes Audio)     |
                                                |                         |
                                                +-------------------------+
                                                             |
                                                             v
                                                +-------------------------+
                                                |                         |
                                                |  Azure Function          |
                                                |  (Processes and Stores   |
                                                |   Transcription Result)  |
                                                |                         |
                                                +-------------------------+
```

### Breakdown of Components:
- **User/Client**: Responsible for uploading the audio files.
- **Azure Blob Storage**: Stores the audio files in a container and triggers an Azure Function when a file is uploaded.
- **Azure Function**: Retrieves the audio file from Blob Storage, passes it to the Speech SDK, processes the transcription, and stores the result.
- **Azure Speech Service**: Transcribes the audio file into text.

This textual diagram represents the flow of data and interactions between the components of your project.


-------
------

### Step 1: Set Up an **Azure Speech Service** Resource for Speaker Diarization

Before enabling speaker diarization, you need to set up the **Azure Speech Service** in your Azure environment. This service will handle the transcription and speaker identification.

#### 1.1: **Create a Speech Service Resource on Azure**

1. **Sign in** to the [Azure Portal](https://portal.azure.com/).
2. In the **search bar** at the top, type "Speech" and select **Speech** from the dropdown under **Cognitive Services**.
3. Click on **Create** to create a new Speech resource.
4. Fill in the necessary information:
   - **Subscription**: Select your Azure subscription.
   - **Resource Group**: Create a new resource group or use an existing one.
   - **Region**: Select a region closest to your location.
   - **Name**: Give your Speech resource a unique name.
   - **Pricing Tier**: Select the **Free F0** pricing tier for basic testing, or choose a paid tier if you expect high usage.

5. Click **Review + Create** and then **Create** to finalize the setup.

#### 1.2: **Retrieve the Speech Service API Key and Endpoint**

After creating the Speech Service resource, you will need the **API Key** and **Endpoint** to connect your Python application to the Speech Service.

1. Go to the **Speech Resource** you just created.
2. In the left-hand menu, click on **Keys and Endpoint** under **Resource Management**.
3. Copy the **Key 1** and **Region** values, which you'll use in your application code.

At this point, you should have:
- **Speech Service Key** (API Key) d30e6131dfc7495081b0f434511df004
- **Region** (where the service is hosted) eastus
- https://eastus.api.cognitive.microsoft.com/

Once you've set up the Speech Service and have the API key and region, let me know, and we can proceed to the next step, where we'll start writing the code to integrate this service.

### Step 2: Set Up Azure Blob Storage for Audio File Upload

In this step, you'll create an **Azure Blob Storage** container to store the audio files that need to be transcribed.

#### 2.1: **Create a Storage Account**

1. In the [Azure Portal](https://portal.azure.com/), search for **Storage accounts** in the search bar.
2. Click on **Create** to create a new storage account.
3. Fill in the required fields:
   - **Subscription**: Select your subscription.
   - **Resource group**: Use the same resource group as your Speech Service (or create a new one).
   - **Storage account name**: Enter a unique name for your storage account.
   - **Region**: Select the region closest to your location (preferably the same as your Speech Service).
   - **Performance**: Standard.
   - **Redundancy**: Locally redundant storage (LRS) for the most cost-effective solution.

4. Click **Review + Create**, and once validated, click **Create** to provision the storage account.

#### 2.2: **Create a Blob Container**

1. After your storage account is created, navigate to the storage account dashboard.
2. Under **Data storage**, click **Containers**.
3. Click **+ Container** to create a new container.
4. Name the container, for example, `audio-files`.
5. Set the **Public Access Level** to **Private** to restrict access to authorized users.
6. Click **Create**.

#### 2.3: **Retrieve Storage Account Connection String**

You'll need the connection string to interact with Azure Blob Storage from your Python application.

1. In your storage account, go to **Access keys** under **Security + networking**.
2. Copy the **Connection string** for **key1**. You'll use this in the Python code to upload and retrieve audio files.
			Connection string: DefaultEndpointsProtocol=https;AccountName=trascriptionappstorage;AccountKey=QASes7F+fWoimu2/ZvLJDvpsh3m2F+v0gaJOp3sHLJVshmLBHlKY0/H1Zd9QobBgWw42JSrvwKBl+ASt5djIKQ==;EndpointSuffix=core.windows.net

				Key: QASes7F+fWoimu2/ZvLJDvpsh3m2F+v0gaJOp3sHLJVshmLBHlKY0/H1Zd9QobBgWw42JSrvwKBl+ASt5djIKQ==
			

Once you have the Blob Storage container ready and you've noted the connection string, let me know. We'll move on to integrating the Blob Storage functionality in Python to upload audio files and prepare for transcription.

To merge your code with the **Azure Blob Storage** upload and Azure Speech-based transcription solution, we will follow these steps:

### Objective of the Merge:
- **Upload Audio Files to Azure Blob Storage**: Use the existing Python script to upload audio files to Azure Blob Storage.
- **Trigger Transcription Using Azure Functions**: When the audio is uploaded, an Azure Function will trigger and run a transcription process using Azure Speech Service.
- **Integrate Blob Upload and Transcription**: Modify your existing script to handle the audio upload to Azure Blob and initiate the transcription process using Azure's SDK.

### Step 1: Merge the Blob Upload Code

Start by integrating the **Azure Blob Storage upload** functionality into your existing Python code so that it uploads the audio file before processing the transcription.

Here’s how to integrate it:

1. **Define Blob Upload Logic**: Add the blob upload logic to your existing script. The code below will be used to upload audio files to Blob Storage, followed by a call to download and transcribe the audio.

Here’s the merged code:

```python
from azure.storage.blob import BlobServiceClient
import io
import os
import warnings
import time
import datetime
import requests
import torch
import whisper
import numpy as np
from pydub import AudioSegment
from pydub.silence import detect_silence
from colorama import Fore, Style, init
import re

# Azure Blob Storage configuration
connection_string = "<YOUR_AZURE_STORAGE_CONNECTION_STRING>"
container_name = "audio-files"

# Function to upload audio to Azure Blob Storage
def upload_audio_to_blob(file_path):
    try:
        # Initialize the BlobServiceClient
        blob_service_client = BlobServiceClient.from_connection_string(connection_string)
        
        # Create a blob client using the audio file name
        blob_name = os.path.basename(file_path)
        blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)
        
        # Upload the audio file to the Blob
        with open(file_path, "rb") as data:
            blob_client.upload_blob(data)
        
        print(f"Uploaded {file_path} to container {container_name} as {blob_name}.")
    
    except Exception as e:
        print(f"Error uploading file to Blob: {e}")

# Function to fetch data from the API using the provided parameters
def fetch_data(api_url, params):
    """Fetch data from API using the specified parameters."""
    try:
        # Debug: print the actual request URL with parameters
        print(f"Request URL: {requests.Request('GET', api_url, params=params).prepare().url}")

        response = requests.get(api_url, params=params)
        response.raise_for_status()  # Raise an error for bad status codes

        # Debug: print the raw response content
        print(f"Response status code: {response.status_code}")
        print(f"Response content: {response.text}")

        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"Error fetching data from API: {e}")
        return []

# Function to download audio files
def download_audio(url):
    """Download audio file and return it as a BytesIO object."""
    if not url:
        print("No audio URL provided.")
        return None
    response = requests.get(url)
    if response.status_code != 200:
        print(f"Failed to stream audio from {url}. Status: {response.status_code}")
        return None
    return io.BytesIO(response.content)

# Function to calculate call duration in seconds
def calculate_duration(call_in_time, call_out_time):
    """Calculate call duration in seconds given the call in and out times in AM/PM format."""
    call_in = datetime.datetime.strptime(call_in_time, "%I:%M:%S %p")
    call_out = datetime.datetime.strptime(call_out_time, "%I:%M:%S %p")
    duration = call_out - call_in
    return duration

# Function to convert audio to numpy array
def audio_to_numpy(audio):
    """Convert pydub AudioSegment to numpy array."""
    samples = audio.get_array_of_samples()
    return np.array(samples).astype(np.float32) / 32768.0  # Normalize to [-1.0, 1.0]

# Function to sanitize filenames
def sanitize_filename(name):
    """Sanitize the store name to create a valid file name."""
    return re.sub(r'[^\w\s-]', '', name).replace(' ', '_')

# Function to create the transcription folder on the desktop
def create_transcription_folder():
    """Create the 'Transcription_Results' folder on the desktop if it doesn't exist."""
    desktop_path = os.path.join(os.path.expanduser("~"), "Desktop")
    folder_path = os.path.join(desktop_path, "Transcription_Results")

    if not os.path.exists(folder_path):
        os.makedirs(folder_path)
        print(f"Created folder: {folder_path}")
    else:
        print(f"Folder already exists: {folder_path}")

    return folder_path

# Function to process and transcribe each call
def process_call(order_id, audio_file_path):
    """Process and transcribe call for the given order ID."""
    # Upload the audio file to Azure Blob Storage
    upload_audio_to_blob(audio_file_path)

    # Load Whisper model here, so it is ready for each order ID
    try:
        model = whisper.load_model("base")
        print("Whisper model loaded.")
    except Exception as e:
        print(f"Error loading Whisper model: {e}")
        return

    # Define the query parameters to search by `orderId` only
    params = {
        "page": 1,       # Page number
        "limit": 1,      # Limit number of results (e.g., 1)
        "orderId": order_id  # The Order ID to search for
    }

    # Define the API endpoint
    api_url = "https://telephony.ordrai.com/calls"

    # Start timer for measuring script execution time
    start_time = time.time()

    # Fetch the data
    response = fetch_data(api_url, params)

    # Ensure the response is a dictionary or list and check for 'data'
    if isinstance(response, dict) and 'data' in response:
        call_data = response['data']
    elif isinstance(response, list):
        call_data = response
    else:
        print(f"Unexpected API response format: {type(response)}")
        call_data = []

    if not call_data:
        print(f"No data found for Order ID: {order_id}")
        return None

    # Step 4: Process the retrieved call data
    for call in call_data:
        print(f"Processing data for call ID: {call.get('callSid')}")

        store_date = call.get('storeDate')
        call_time = call.get('callTimeAMPM')
        store_name = call.get('storeName')
        agent_name = call.get('agentName', 'Unknown')
        watched_by = call.get('watchedBy', 'No One')
        customer_phone = call.get('custId')
        order_id = call.get('orderId', 'N/A')  # Handle empty orderId
        call_type = call.get('orderType')
        queue_type = call.get('queueName')
        call_status = call.get('callStatus')
        call_in_time = call.get('callInTimeAMPM')
        call_out_time = call.get('callOutTimeAMPM')
        call_duration = calculate_duration(call_in_time, call_out_time) if call_in_time and call_out_time else 'N/A'

        # Sanitize store name for the filename
        sanitized_store_name = sanitize_filename(store_name)

        # Create the transcription folder on the desktop if it doesn't exist
        transcription_folder = create_transcription_folder()

        # Generate the full path for the HTML file
        output_file_path = os.path.join(transcription_folder, f"{sanitized_store_name}.html")

        # HTML content initialization for the new transcription
        new_html_content = f"""
        <h2>New Transcription Results for {store_name}</h2>
        <h3><strong>Audio File for Store {store_name} ({order_id})</strong></h3>
        <p>
            <strong>Date:</strong> {store_date}<br>
            <strong>Time:</strong> {call_time}<br>
            <strong>Store Name:</strong> {store_name}<br>
            <strong>Agent Name:</strong> {agent_name}<br>
            <strong>Watched By:</strong> {watched_by}<br>
            <strong>Customer Phone:</strong> {customer_phone}<br>
            <strong>Order ID:</strong> {order_id}<br>
            <strong>Call Type:</strong> {call_type}<br>
            <strong>Queue Type:</strong> {queue_type}<br>
            <strong>Call Status:</strong> {call_status}<br>
            <strong>Call in Time:</strong> {call_in_time}<br>
            <strong>Call out Time:</strong> {call_out_time}<br>
            <strong>Call Duration:</strong> {call_duration}
        </p>
        <ol>
        """

        # Download audio and process transcription and silence detection
        recording_url = call.get('recordingURL')
        if not recording_url:
            print(f"No audio URL for call with call ID: {call.get('callSid')}. Skipping.")
            continue

        audio_content = download_audio(recording_url)
        if audio_content:
            try:
                # Load the audio using PyDub directly from the BytesIO object
                audio = AudioSegment.from_mp3(audio_content)

                # Analyze the average volume of the audio
                average_db

fs = audio.dBFS

                # Test different silence detection thresholds and lengths
                silence_thresh = average_dbfs - 14
                min_silence_len = 1000

                # Detect silence ranges in the audio file
                silence_ranges = detect_silence(audio, min_silence_len=min_silence_len, silence_thresh=silence_thresh)

                # Convert silence ranges to seconds for easier readability
                silence_ranges_seconds = [(start / 1000, end / 1000, (end - start) / 1000) for start, end in silence_ranges]

                # Convert audio to numpy array for Whisper
                audio_np = audio_to_numpy(audio)

                # Transcribe the audio using Whisper with CUDA support
                result = model.transcribe(audio_np)

                transcription = result['text']
                segments = result['segments']

                combined_segments = []
                current_index = 0
                call_silences_after_target = 0
                target_phrase_end_time = None

                # Iterate through each segment and match with silence
                for segment in segments:
                    combined_segments.append(f'"{segment["text"].strip()}"')

                for segment in combined_segments:
                    new_html_content += f"<li>{segment}</li>\n"
                new_html_content += "</ol>\n"

            except Exception as e:
                print(f"Error processing audio for call with ID {call.get('callSid')}: {str(e)}")
                continue

        # Calculate the duration of the script
        end_time = time.time()
        duration_minutes = (end_time - start_time) / 60

        # Add the duration to the HTML content
        new_html_content += f"<h2><strong>Script Execution Time:</strong> {duration_minutes:.2f} minutes</h2>\n"

        # Check if the file exists
        if os.path.exists(output_file_path):
            # If the file exists, append new content to the bottom
            with open(output_file_path, "a", encoding="utf-8") as f:
                f.write(new_html_content)
            print(f"New transcription appended to {output_file_path}.")
        else:
            # If the file doesn't exist, create a new file with HTML structure
            full_html_content = f"""
            <!DOCTYPE html>
            <html lang="en">
            <head>
                <meta charset="UTF-8">
                <title>Transcription Results for {store_name}</title>
                <style>
                    .long-silence {{
                        color: red;
                        font-weight: bold;
                    }}
                    .medium-silence {{
                        color: darkorange;
                        font-weight: bold;
                    }}
                    .short-silence {{
                        color: green;
                        font-weight: bold;
                    }}
                </style>
            </head>
            <body>
            """ + new_html_content + """
            </body>
            </html>
            """
            with open(output_file_path, "w", encoding="utf-8") as f:
                f.write(full_html_content)
            print(f"Transcription results have been saved to {output_file_path}.")
    

### Main Loop to Process Multiple `orderId`s
while True:
    order_ids = input("Enter the Order IDs separated by commas (or type 'exit' to stop): ")

    if order_ids.lower() == "exit":
        print("Exiting the script.")
        break

    # Split the input by commas and strip any extra whitespace
    order_ids_list = [order_id.strip() for order_id in order_ids.split(",")]

    # Loop through each Order ID and process it individually
    for order_id in order_ids_list:
        process_call(order_id, "sample-audio.mp3")  # Specify the path of your audio file
```

### Summary of Changes:
- **Blob Upload**: The `upload_audio_to_blob()` function is added and called inside `process_call()` before starting transcription, ensuring that audio files are uploaded to Azure Blob Storage.
- **Audio File Path**: The function accepts an audio file path as an argument for easier testing and modification.

Let me know if you'd like to proceed with further steps, like adding Azure Function triggers for automatic transcription or integrating speaker diarization!

### Step 4: Set Up an Azure Function to Automatically Trigger on New Blob Uploads

Now that you have your audio file uploaded to Azure Blob Storage and your code integrated with environment variables, the next step is to set up an **Azure Function** that will automatically trigger when a new audio file is uploaded. This Azure Function will handle transcription using the **Azure Speech SDK** with **Speaker Diarization**.

Here’s how to configure the Azure Function:

### Step 4.1: Create a Python Azure Function with Blob Trigger

#### 4.1.1: Install Azure Functions Core Tools

Before you can develop and test Azure Functions locally, you’ll need to install **Azure Functions Core Tools**:

- **Windows**: Install it via npm:
  ```bash
  npm install -g azure-functions-core-tools@4 --unsafe-perm true
  ```

- **macOS/Linux**: Follow the [official documentation](https://docs.microsoft.com/azure/azure-functions/functions-run-local).

#### 4.1.2: Create a New Function App Project

1. Open **Visual Studio Code** or your preferred IDE.
2. Install the **Azure Functions** extension if you haven’t already.
3. Create a new Function App project by using the Azure Functions extension:
    - Select **Create New Project** from the extension.
    - Choose a folder for your project.
    - Select **Python** as the runtime stack.
    - Select your preferred Python version.
    - Choose **Blob Trigger** as the type of function.

4. In the trigger configuration:
    - **Blob Path**: Enter `audio-files/{name}`. This tells Azure Functions to listen for new blobs in the `audio-files` container.
    - **Storage Account**: Use the connection string environment variable (`AZURE_STORAGE_CONNECTION_STRING`) or create a new storage account if necessary.

#### 4.1.3: Install Python Dependencies

You will need to install the **Azure Speech SDK** in your Azure Function environment to handle transcription.

1. Open your function's directory in a terminal.
2. Install the Azure Speech SDK using pip:
   ```bash
   pip install azure-cognitiveservices-speech
   ```
3. Add this to your `requirements.txt` so it’s included when the function is deployed:
   ```
   azure-cognitiveservices-speech
   ```

#### 4.1.4: Azure Function Code for Transcription

Here’s the core Azure Function code to transcribe the uploaded audio files using **Azure Speech SDK** and **Speaker Diarization**.

```python
import os
import logging
import azure.functions as func
import azure.cognitiveservices.speech as speechsdk

# Retrieve Speech API credentials from environment variables
speech_key = os.getenv("AZURE_SPEECH_KEY")
service_region = os.getenv("AZURE_REGION")

# Check if environment variables are set
if not speech_key or not service_region:
    raise ValueError("Azure Speech Key or Region not found in environment variables.")

def main(blob: func.InputStream):
    logging.info(f"Processing blob: Name={blob.name}, Size={blob.length} bytes")

    # Create the Speech configuration
    speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)

    # Enable speaker diarization
    speech_config.set_property_by_name("DifferentiateGuestSpeakers", "True")

    # Transcribe the audio using Azure Speech SDK
    audio_input = speechsdk.audio.AudioConfig(stream=blob)
    recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_input)

    # Perform transcription
    result = recognizer.recognize_once()

    if result.reason == speechsdk.ResultReason.RecognizedSpeech:
        logging.info(f"Transcription: {result.text}")
    else:
        logging.error(f"Speech not recognized: {result.reason}")

    # Process diarization results (optional)
    if result.speaker_id:
        logging.info(f"Speaker {result.speaker_id}: {result.text}")
```

### Step 4.2: Configure Environment Variables for Azure Functions

1. Go to the **Azure Portal** and open your Function App.
2. In the **Configuration** section, set the following environment variables:
    - `AZURE_SPEECH_KEY`: Your Azure Speech Service API Key.
    - `AZURE_REGION`: The region of your Speech Service.
    - Ensure that the **Blob Storage connection string** is also set.

### Step 4.3: Deploy and Test the Function

1. In **Visual Studio Code**, use the **Azure Functions extension** to deploy the Function App to your Azure environment:
    - Right-click on your function folder and select **Deploy to Function App**.
    - Follow the prompts to deploy to an existing or new Function App.

2. Once deployed, the Function will automatically trigger whenever an audio file is uploaded to the `audio-files` container.

3. Upload an audio file to the Blob Storage container either using your existing script or manually through the Azure Portal.

4. Check the logs for your Azure Function in the portal to verify that the transcription and speaker diarization are working.

### Step 4.4: Monitor and Test the Function

- In the Azure portal, navigate to **Monitor** under your Function App to see the execution logs.
- You should see the transcription results along with the detected speakers logged in the output.

### Summary of This Step:
- You created an Azure Function that triggers when a new audio file is uploaded to Blob Storage.
- The function uses **Azure Speech SDK** to transcribe the audio and differentiate between speakers using **speaker diarization**.
- You can monitor the transcription process and logs in the Azure portal.

Let me know once this is done, and we can move to the final steps, such as improving the error handling or storing the transcription results in a persistent data store!


