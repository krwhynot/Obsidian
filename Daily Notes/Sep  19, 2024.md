To help you transcribe local MP3 files using Azure Speech, we’ll need to follow these steps:

### Step 1: Setup Azure Speech Service
If you haven't already set up Azure Speech service, you will need to create a Speech service resource in Azure. Follow the steps below:

1. **Create a Speech Resource:**
   - Go to the [Azure portal](https://portal.azure.com/).
   - Search for **Speech** in the search bar.
   - Select **Speech** from the marketplace.
   - Click **Create**.
   - Select your subscription and resource group.
   - Choose a region, name the resource, and select the pricing tier (free tier is available).
   - Click **Review + Create**.

2. **Obtain Your Azure Speech API Key and Region:**
   - After the resource is created, go to the resource page.
   - Under **Keys and Endpoint**, note down your **Key1** and **Region**. You’ll need these for authentication.

### Step 2: Install the Required Python Libraries
Install the necessary libraries for interacting with the Azure Speech service. Open a terminal or command prompt and run:

```bash
pip install azure-cognitiveservices-speech
```

### Step 3: Write the Python Script for Transcription
Now, we can write a Python script to transcribe your local MP3 files using Azure Speech. Below is the script:

#### Python Script: Transcribe MP3 Files with Azure Speech

```python
import os
import azure.cognitiveservices.speech as speechsdk

# Azure Speech configuration
speech_key = "YOUR_SPEECH_KEY"  # Replace with your Azure Speech key
service_region = "YOUR_SERVICE_REGION"  # Replace with your Azure service region (e.g., eastus)

def transcribe_audio(file_path):
    # Initialize the speech configuration
    speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)

    # Create an audio configuration for the given file
    audio_input = speechsdk.audio.AudioConfig(filename=file_path)

    # Create a recognizer with the given configurations
    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_input)

    print(f"Transcribing file: {file_path}")
    
    # Start the transcription process
    result = speech_recognizer.recognize_once()

    # Check the result
    if result.reason == speechsdk.ResultReason.RecognizedSpeech:
        print("Recognized:", result.text)
        return result.text
    elif result.reason == speechsdk.ResultReason.NoMatch:
        print("No speech could be recognized")
    elif result.reason == speechsdk.ResultReason.Canceled:
        cancellation_details = result.cancellation_details
        print(f"Speech Recognition canceled: {cancellation_details.reason}")
        if cancellation_details.reason == speechsdk.CancellationReason.Error:
            print(f"Error details: {cancellation_details.error_details}")
    return None

def transcribe_all_files_in_directory(directory):
    # Ensure the directory exists
    if not os.path.isdir(directory):
        print(f"The directory {directory} does not exist.")
        return

    # Loop through all the mp3 files in the directory
    for file_name in os.listdir(directory):
        if file_name.endswith(".mp3"):
            file_path = os.path.join(directory, file_name)
            transcript = transcribe_audio(file_path)
            if transcript:
                print(f"Transcript for {file_name}: {transcript}")
            else:
                print(f"Could not transcribe {file_name}")

# Path to the directory containing the mp3 files
directory_path = r"C:\Users\revadmin\Desktop\Projects\test_phone_call_azure_speech\mp3_files"

# Start transcribing all MP3 files in the directory
transcribe_all_files_in_directory(directory_path)
```

### Step 4: Explanation of the Code

- **Azure Speech Configuration:**
  - You need to replace `"YOUR_SPEECH_KEY"` and `"YOUR_SERVICE_REGION"` with the actual values from the Azure Speech resource you created.
  
- **Transcribing Audio:**
  - `transcribe_audio(file_path)`: This function takes the file path of an MP3 file and sends it to the Azure Speech service for transcription.
  - `speechsdk.SpeechRecognizer.recognize_once()`: This method sends the audio to Azure Speech for recognition. The response is checked to see if speech was recognized, and if so, it returns the transcribed text.

- **Looping Through MP3 Files:**
  - The `transcribe_all_files_in_directory(directory)` function loops through all `.mp3` files in the given directory and calls `transcribe_audio()` for each file.

### Step 5: Running the Script

1. Save the script as `transcribe_mp3_files.py` in your project directory.
2. Open a terminal or command prompt, navigate to the project directory, and run:

```bash
python transcribe_mp3_files.py
```

This will start transcribing all MP3 files in the specified directory.

### Need to Convert Mp3 to Wav
Tried to run this script on MP3 but it did not work and got this error message: 
   - pal_string_to_wstring


### How to Convert MP3 to WAV (Optional, but Recommended for Debugging)
While Azure Speech SDK supports MP3, converting to WAV helps avoid some potential issues. To convert the MP3 file to a WAV format using `ffmpeg` (if installed), open a command prompt and run the following:

```bash
ffmpeg -i "C:\Users\revadmin\Desktop\Projects\test_phone_call_azure_speech\mp3_files\CA636d5e5ea296ed2664c77c951f0878d2.mp3" -acodec pcm_s16le -ar 16000 "C:\Users\revadmin\Desktop\Projects\test_phone_call_azure_speech\mp3_files\CA636d5e5ea296ed2664c77c951f0878d2.wav"
```

This converts the MP3 file to a 16-bit, 16 kHz WAV file, which is typically more compatible for speech recognition.

If you don't have `ffmpeg`, you can skip this step and directly test the MP3 file, though using WAV can sometimes help debug issues.

### Step 3: Modify the Python Script to Test This One File

Create or modify a Python script that points to this specific file, transcribes it, and prints the result. Below is a modified version of your script for testing just this file.

#### Python Script to Transcribe a Single File

```python
import azure.cognitiveservices.speech as speechsdk
import os

# Azure Speech configuration
speech_key = "YOUR_SPEECH_KEY"  # Replace with your Azure Speech key
service_region = "YOUR_SERVICE_REGION"  # Replace with your Azure service region (e.g., eastus)

def transcribe_audio(file_path):
    try:
        # Initialize the speech configuration
        speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)
        audio_input = speechsdk.audio.AudioConfig(filename=file_path)
        speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_input)

        print(f"Transcribing file: {file_path}")
        result = speech_recognizer.recognize_once()

        if result.reason == speechsdk.ResultReason.RecognizedSpeech:
            print(f"Transcript: {result.text}")
        elif result.reason == speechsdk.ResultReason.NoMatch:
            print("No speech could be recognized")
        elif result.reason == speechsdk.ResultReason.Canceled:
            cancellation_details = result.cancellation_details
            print(f"Speech Recognition canceled: {cancellation_details.reason}")
            if cancellation_details.reason == speechsdk.CancellationReason.Error:
                print(f"Error details: {cancellation_details.error_details}")
    except Exception as e:
        print(f"An error occurred: {e}")

# Path to the specific MP3 file (or WAV file if you converted it)
file_path = r"C:\Users\revadmin\Desktop\Projects\test_phone_call_azure_speech\mp3_files\CA636d5e5ea296ed2664c77c951f0878d2.mp3"

# Check if file exists before transcribing
if os.path.exists(file_path):
    transcribe_audio(file_path)
else:
    print(f"File not found: {file_path}")
```

### Step 4: Configure API Key and Region
Make sure to replace `YOUR_SPEECH_KEY` and `YOUR_SERVICE_REGION` with the actual values from your Azure Speech resource:

- **speech_key**: Your Azure Speech service API key.
- **service_region**: The region where your Speech resource is located (e.g., `eastus`, `westeurope`).

### Step 5: Run the Script

1. Save the script as `transcribe_single_file.py` in your project directory.
2. Open a terminal or command prompt.
3. Navigate to the folder where the script is saved. For example:

   ```bash
   cd C:\Users\revadmin\Desktop\Projects\test_phone_call_azure_speech
   ```

4. Run the script using your virtual environment's Python interpreter:

   ```bash
   .venv\Scripts\python.exe transcribe_single_file.py
   ```

### Step 6: Review the Output
If the transcription is successful, the console will print something like:

```
Transcribing file: C:\Users\revadmin\Desktop\Projects\test_phone_call_azure_speech\mp3_files\CA636d5e5ea296ed2664c77c951f0878d2.mp3
Transcript: Hello, I would like to order a pizza.
```

If there are any issues (like the error you encountered previously), it will show a more detailed error message.

### Step 7: Troubleshooting

Had an issue where only the first 7 seconds of the file are transcribed might be due to the default behavior of the `recognize_once()` method in the Azure Speech SDK. This method is designed to transcribe only a single phrase or sentence, which usually lasts just a few seconds. For longer audio files, you should use **continuous recognition**, which listens for the entire audio duration.

### Solution: Use Continuous Recognition

To handle longer files and transcribe the entire audio, we can modify the script to use **continuous recognition** instead of `recognize_once()`. Continuous recognition will transcribe the entire file and trigger events as it processes different parts of the audio.

Here's how to modify the script to handle continuous transcription:

### Updated Python Script for Continuous Recognition

```python
import azure.cognitiveservices.speech as speechsdk
import os

# Azure Speech configuration
speech_key = "YOUR_SPEECH_KEY"  # Replace with your Azure Speech key
service_region = "YOUR_SERVICE_REGION"  # Replace with your Azure service region (e.g., eastus)

def transcribe_audio(file_path):
    try:
        # Initialize the speech configuration
        speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)
        audio_input = speechsdk.audio.AudioConfig(filename=file_path)
        speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_input)

        print(f"Transcribing file: {file_path}")

        # Create a list to store the transcriptions
        transcription = []

        # Event handler for recognized speech
        def recognized_handler(evt):
            print(f"Recognized: {evt.result.text}")
            transcription.append(evt.result.text)

        # Connect the recognized event to the handler
        speech_recognizer.recognized.connect(recognized_handler)

        # Start continuous recognition
        speech_recognizer.start_continuous_recognition()

        # Wait until the recognition is finished
        print("Transcribing...")
        speech_recognizer.recognize_once_async().get()

        # Stop recognition after processing the entire file
        speech_recognizer.stop_continuous_recognition()

        # Print the full transcription
        full_transcription = ' '.join(transcription)
        print(f"Full Transcript: {full_transcription}")

    except Exception as e:
        print(f"An error occurred: {e}")

# Path to the WAV file
file_path = r"C:\Users\revadmin\Desktop\Projects\test_phone_call_azure_speech\mp3_files\CA636d5e5ea296ed2664c77c951f0878d2.wav"

# Check if file exists before transcribing
if os.path.exists(file_path):
    transcribe_audio(file_path)
else:
    print(f"File not found: {file_path}")
```

### Key Changes:
1. **Event-Driven Continuous Recognition**: 
   - Instead of using `recognize_once()`, the script uses `start_continuous_recognition()` to continuously process and transcribe the audio file.
   - The `recognized_handler()` function is called whenever a portion of speech is recognized, and it appends the recognized text to a `transcription` list.

2. **Handling Full Transcription**:
   - After all speech in the audio is processed, the `stop_continuous_recognition()` method is called to stop the recognition process.
   - The full transcription is assembled by joining the text fragments stored in the `transcription` list.

### Step-by-Step Execution:

1. **Save the Script**: 
   Save the updated script as `transcribe_continuous.py`.

2. **Run the Script**:
   In your virtual environment, run the script:

   ```bash
   .venv\Scripts\python.exe transcribe_continuous.py
   ```

3. **Output**:
   The output will show intermediate recognized parts of the audio, and at the end, it will print the full transcript.

   Example output:
   ```
   Transcribing file: C:\Users\revadmin\Desktop\Projects\test_phone_call_azure_speech\mp3_files\CA636d5e5ea296ed2664c77c951f0878d2.wav
   Transcribing...
   Recognized: Hello, I would like to order a pizza.
   Recognized: I want a large pepperoni pizza.
   Recognized: Can you also add extra cheese, please?
   Full Transcript: Hello, I would like to order a pizza. I want a large pepperoni pizza. Can you also add extra cheese, please?
   ```

### Explanation of the Code:
- **Event Handler (`recognized_handler`)**: 
   - This function is triggered each time the recognizer identifies a part of the audio file. It adds each recognized phrase to the `transcription` list.
   
- **Continuous Recognition**: 
   - The `start_continuous_recognition()` function starts the transcription of the entire audio file.
   - The process stops after the full audio is transcribed using `stop_continuous_recognition()`.

### Summary:
- Use **continuous recognition** for longer audio files to avoid cutting off transcription at 7 seconds.
- The script will now transcribe the entire audio file and print the full transcript at the end.

It's a good practice to **avoid hardcoding sensitive credentials** (like your Azure Speech API key) directly in your code, especially when pushing to version control systems like GitHub. Here's how you can securely manage your credentials without exposing them:

### Step 1: Use Environment Variables

One of the safest and most common ways to handle credentials is by storing them in **environment variables**. This way, you don't include them directly in your code.

#### 1.1 Set Environment Variables

1. **On Windows (PowerShell):**
   You can set environment variables using PowerShell like this:

   ```powershell
   $env:AZURE_SPEECH_KEY = "your_speech_key"
   $env:AZURE_SERVICE_REGION = "your_service_region"
   ```

2. **On Windows (Command Prompt):**
   You can set environment variables using the `set` command:

   ```cmd
   set AZURE_SPEECH_KEY=your_speech_key
   set AZURE_SERVICE_REGION=your_service_region
   ```

3. **Set Environment Variables in `.env` File (with `python-dotenv`)**:
   - Install the `python-dotenv` package, which loads environment variables from a `.env` file:

     ```bash
     pip install python-dotenv
     ```

   - Create a `.env` file in your project directory:

     ```txt
     AZURE_SPEECH_KEY=your_speech_key
     AZURE_SERVICE_REGION=your_service_region
     ```

   - Modify your script to load environment variables from the `.env` file:

     ```python
     from dotenv import load_dotenv
     import os

     # Load environment variables from .env file
     load_dotenv()

     # Access the variables
     speech_key = os.getenv("AZURE_SPEECH_KEY")
     service_region = os.getenv("AZURE_SERVICE_REGION")
     ```

#### 1.2 Modify Your Python Script

Instead of hardcoding your credentials, use the environment variables in your Python script:

```python
import os

# Fetch credentials from environment variables
speech_key = os.getenv("AZURE_SPEECH_KEY")
service_region = os.getenv("AZURE_SERVICE_REGION")

# Check if the credentials are loaded correctly
if not speech_key or not service_region:
    raise Exception("Azure Speech credentials not found. Make sure to set AZURE_SPEECH_KEY and AZURE_SERVICE_REGION.")

# The rest of your transcription code follows
```

### Step 2: Add `.env` to `.gitignore`
If you decide to use a `.env` file, make sure to prevent it from being uploaded to GitHub. Do this by adding it to your `.gitignore` file.

1. Open (or create) `.gitignore` in your project directory.
2. Add the following line to ignore the `.env` file:

   ```
   .env
   ```

This ensures the `.env` file won't be committed to GitHub.

### Step 3: Commit Your Changes Safely
Now that your credentials are securely stored in environment variables, you can commit your code to GitHub without exposing sensitive information.

1. **Check if your `.env` file is excluded** by running the following command:

   ```bash
   git status
   ```

   Ensure the `.env` file doesn't appear in the list of files to be committed.

2. **Commit your changes**:

   ```bash
   git add .
   git commit -m "Refactor: Moved Azure credentials to environment variables"
   git push
   ```

### Step 4: Set Environment Variables in the Deployment Environment
If you're deploying your code to a cloud service, CI/CD pipeline, or a server, make sure to set the environment variables in those environments as well:

- **Azure App Service**: Set environment variables in the App Settings section.
- **GitHub Actions**: Use GitHub secrets to store environment variables.
- **CI/CD Pipeline**: Add environment variables in the pipeline settings.

By using environment variables, you ensure your credentials stay secure and are not accidentally pushed to version control systems like GitHub.

### Troubleshooting 
Got this error: 
```
An error occurred: Exception with error code: 
[CALL STACK BEGIN]

    > pal_string_to_wstring
    - dialog_service_connector_turn_status_received_get_status
    - recognizer_recognize_once_async
    - ffi_prep_go_closure
    - ffi_call_go
    - ffi_call
    - 00007FFADD683A47 (SymFromAddr() error: Attempt to access invalid address.)
    - 00007FFADD6836AC (SymFromAddr() error: Attempt to access invalid address.)
    - PyObject_Call
    - PyEval_EvalFrameDefault
    - PyFunction_Vectorcall
    - PyVectorcall_Call
    - PyObject_Call
    - PyEval_EvalFrameDefault
    - Py_CheckFunctionResult
```


The error you're encountering, specifically `SPXERR_SWITCH_MODE_NOT_ALLOWED`, is related to using the `recognize_once_async()` method after starting continuous recognition with `start_continuous_recognition()`. These two methods are not compatible because one is for **single-shot transcription** and the other for **continuous transcription**. You cannot mix them within the same session.

### Fix:
To handle the continuous transcription correctly, you should remove the call to `recognize_once_async()` and instead rely on continuous transcription to capture all recognized speech.

### Updated Code Without `recognize_once_async()`

```python
import azure.cognitiveservices.speech as speechsdk
import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Access the variables
speech_key = os.getenv("AZURE_SPEECH_KEY")
service_region = os.getenv("AZURE_SERVICE_REGION")

def transcribe_audio(file_path):
    try:
        # Initialize the speech configuration
        speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)
        audio_input = speechsdk.audio.AudioConfig(filename=file_path)
        speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_input)

        print(f"Transcribing file: {file_path}")

        # Create a list to store the transcriptions
        transcription = []

        # Event handler for recognized speech
        def recognized_handler(evt):
            print(f"Recognized: {evt.result.text}")
            transcription.append(evt.result.text)

        # Connect the recognized event to the handler
        speech_recognizer.recognized.connect(recognized_handler)

        # Event handler for session stop (when the transcription finishes)
        def session_stopped_handler(evt):
            print("Session stopped.")
            speech_recognizer.stop_continuous_recognition()

        # Connect the session stopped event to the handler
        speech_recognizer.session_stopped.connect(session_stopped_handler)

        # Start continuous recognition
        speech_recognizer.start_continuous_recognition()

        # Wait for the transcription to complete
        print("Transcribing... Press Ctrl+C to stop manually if needed.")
        while True:
            pass  # Wait indefinitely or until manual interruption (Ctrl+C)

        # Full transcription output
        full_transcription = ' '.join(transcription)
        print(f"Full Transcript: {full_transcription}")

    except Exception as e:
        print(f"An error occurred: {e}")

# Path to the WAV file
file_path = r"C:\Users\revadmin\Desktop\Projects\test_phone_call_azure_speech\mp3_files\CA636d5e5ea296ed2664c77c951f0878d2.wav"

# Check if file exists before transcribing
if os.path.exists(file_path):
    transcribe_audio(file_path)
else:
    print(f"File not found: {file_path}")
```

### Key Fixes:
1. **Removed `recognize_once_async()`**:
   - The error occurred because `recognize_once_async()` was being called after `start_continuous_recognition()`, which is not allowed.
   - We now rely solely on continuous transcription.

2. **Session Stopped Handling**:
   - I added an event handler (`session_stopped_handler`) to handle the end of the transcription session properly. It will stop the continuous recognition once the session ends.

3. **Manual Termination**:
   - The script will now continuously listen for transcription, and you can terminate it manually by pressing `Ctrl + C` when you're satisfied that the transcription is complete.

### How to Run:
1. Run the script in your virtual environment as before:
   
   ```bash
   .venv\Scripts\python.exe transcribe_continuous.py
   ```

2. The script will continuously transcribe the audio. To stop it, you can manually interrupt by pressing `Ctrl + C` once you believe the transcription is complete.

### Next Steps:
- If the session stops and finishes transcribing, you should see the final transcript in the terminal.
- If the audio file is long or the transcription continues for a while, you can stop it when necessary.

Let me know if this solution works for you or if any other issues arise!

To make your transcription solution work on any computer, even if `ffmpeg` is not installed, we need to modify the approach so that audio file format conversion or preparation can be handled programmatically, without relying on external dependencies like `ffmpeg`. Here's how you can achieve that:

### Options to Avoid External Dependencies:
1. **Use Python Libraries for Audio Processing**: There are Python libraries that allow you to process and convert audio formats without needing external tools like `ffmpeg`. Two popular libraries are:
   - **Pydub**: For audio file format conversion.
   - **Soundfile**: For handling WAV files directly.

2. **Let Azure Handle the MP3**: The Azure Speech SDK already supports MP3 files, so ideally, if the MP3 file is in the correct format, the SDK should be able to handle it. We’ll focus on ensuring that the MP3 files are compatible.

### Solution Using `pydub` to Convert MP3 to WAV Without `ffmpeg`

`pydub` can handle MP3 to WAV conversion, and it includes built-in functionality for handling audio without `ffmpeg` for certain operations. However, to work with MP3 files, it does require an additional backend library, `simpleaudio` for playing files and optionally `ffmpeg` or `libav`. But we can still try to rely on built-in Python functionality to perform conversions.

### Step 1: Install `pydub` and `simpleaudio`
First, you'll need to install `pydub` and `simpleaudio` libraries in your Python environment.

```bash
pip install pydub simpleaudio
```

### Step 2: Modify Your Script to Convert MP3 to WAV Using `pydub`

Here's an updated version of the script that uses `pydub` to convert the MP3 file to a WAV file programmatically. The conversion will happen in-memory, so there will be no need to store temporary files on disk.

#### Updated Python Script:

```python
import azure.cognitiveservices.speech as speechsdk
import os
from dotenv import load_dotenv
from pydub import AudioSegment

# Load environment variables from .env file
load_dotenv()

# Access the variables
speech_key = os.getenv("AZURE_SPEECH_KEY")
service_region = os.getenv("AZURE_SERVICE_REGION")

def convert_mp3_to_wav(mp3_path):
    """Convert MP3 file to WAV format using pydub."""
    try:
        audio = AudioSegment.from_mp3(mp3_path)  # Load MP3 file
        wav_path = mp3_path.replace(".mp3", ".wav")
        audio.export(wav_path, format="wav")  # Export to WAV format
        return wav_path
    except Exception as e:
        print(f"Error converting MP3 to WAV: {e}")
        return None

def transcribe_audio(file_path):
    try:
        # Convert MP3 to WAV if needed
        if file_path.endswith(".mp3"):
            file_path = convert_mp3_to_wav(file_path)
            if not file_path:
                raise Exception("MP3 to WAV conversion failed.")

        # Initialize the speech configuration
        speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)
        audio_input = speechsdk.audio.AudioConfig(filename=file_path)  # MP3/WAV file can be used here
        speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_input)

        print(f"Transcribing file: {file_path}")

        # Create a list to store the transcriptions
        transcription = []

        # Event handler for recognized speech
        def recognized_handler(evt):
            print(f"Recognized: {evt.result.text}")
            transcription.append(evt.result.text)

        # Connect the recognized event to the handler
        speech_recognizer.recognized.connect(recognized_handler)

        # Event handler for session stop (when the transcription finishes)
        def session_stopped_handler(evt):
            print("Session stopped.")
            speech_recognizer.stop_continuous_recognition()

        # Connect the session stopped event to the handler
        speech_recognizer.session_stopped.connect(session_stopped_handler)

        # Start continuous recognition
        speech_recognizer.start_continuous_recognition()

        # Wait for the transcription to complete
        print("Transcribing... Press Ctrl+C to stop manually if needed.")
        while True:
            pass  # Wait indefinitely or until manual interruption (Ctrl+C)

        # Full transcription output
        full_transcription = ' '.join(transcription)
        print(f"Full Transcript: {full_transcription}")

    except Exception as e:
        print(f"An error occurred: {e}")

# Path to the MP3 file
file_path = r"C:\Users\revadmin\Desktop\Projects\test_phone_call_azure_speech\mp3_files\CA636d5e5ea296ed2664c77c951f0878d2.mp3"

# Check if file exists before transcribing
if os.path.exists(file_path):
    transcribe_audio(file_path)
else:
    print(f"File not found: {file_path}")
```

### Key Changes in the Script:
1. **MP3 to WAV Conversion with `pydub`**: 
   - The function `convert_mp3_to_wav(mp3_path)` uses `pydub` to load an MP3 file and convert it into WAV format in memory. The `wav_path` is generated from the MP3 filename by replacing `.mp3` with `.wav`.
   
2. **Automatic Format Handling**:
   - The script checks if the input file is an MP3 file. If it is, it converts it to WAV before passing it to Azure Speech SDK.
   - If the file is already in WAV format, the script proceeds directly to transcription.

### Step 3: Run the Script
Once you've installed the necessary libraries and updated the script, run it as you normally would:

```bash
.venv\Scripts\python.exe main_mp3_format.py
```

### Advantages of this Approach:
- **No Need for `ffmpeg`**: By using `pydub` and `simpleaudio`, you can process audio files without relying on external dependencies like `ffmpeg`.
- **Cross-platform**: This solution will work on any system that can install Python and the `pydub`/`simpleaudio` libraries, making it more portable.

### Alternative: Handle MP3 Files Directly
If you want to avoid converting the file entirely and make Azure handle the MP3 files directly, you can still ensure that your MP3 files are in a compatible format (e.g., using a constant bit rate). However, if some MP3 files continue to give issues due to their encoding, converting them in memory as demonstrated here can be a reliable fallback.

Let me know how this works or if you need further customization!

### Troubleshooting
When ran code with pydub the transcribing was slightly off: saying "Jet bot" transcribing "JetBlue"

The issue you're encountering—where "Jet Bot" is incorrectly transcribed as "JetBlue"—can happen with speech-to-text systems when they misinterpret words or phrases. This can occur due to a variety of factors, such as:

- **Pronunciation similarities**: Words that sound similar may be transcribed incorrectly.
- **Language model biases**: The Azure Speech service may have a tendency to recognize more common phrases or brand names like "JetBlue" over less common ones like "Jet Bot".
- **Audio quality**: Background noise, low-quality audio, or unclear pronunciation can affect transcription accuracy.

### Solutions to Improve Accuracy

Here are several ways you can improve the accuracy of the transcription:

### 1. **Use Custom Speech Recognition with Azure Custom Speech**

Azure offers **Custom Speech** services, allowing you to train the model with specific words or phrases that are unique to your business. For example, if "Jet Bot" is a term that will be frequently mentioned in your recordings, you can create a custom model to help Azure recognize it more accurately.

#### Steps to Create a Custom Model:
- Go to the [Azure Custom Speech portal](https://speech.microsoft.com/portal/).
- Create a **Custom Speech** project.
- Upload **transcribed audio samples** (if available) where the term "Jet Bot" is correctly used. The more examples you provide, the better the model will learn to recognize it.
- Train the custom model using these samples.

Once trained, you can integrate this custom model with your application by specifying the **custom speech endpoint** in your code.

### Example: Using a Custom Speech Model in Your Code

```python
# Use the custom speech model endpoint in the SpeechConfig
speech_config = speechsdk.SpeechConfig(
    subscription=speech_key,
    region=service_region,
    endpoint_id="YOUR_CUSTOM_MODEL_ENDPOINT"
)
```

You can find the **endpoint ID** in the Azure Custom Speech portal after creating and training your model.

### 2. **Use Phrase Lists to Boost Recognition of Specific Terms**

Azure Speech SDK allows you to boost the recognition of certain words or phrases using **phrase lists**. You can add custom terms like "Jet Bot" to a phrase list to improve their transcription accuracy.

Here's how you can add a phrase list to the recognizer:

#### Example: Using a Phrase List in Azure Speech SDK

```python
def transcribe_audio(file_path):
    try:
        # Initialize the speech configuration
        speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)
        audio_input = speechsdk.audio.AudioConfig(filename=file_path)
        speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_input)

        # Create a phrase list to boost recognition of specific terms
        phrase_list = speechsdk.PhraseListGrammar.from_recognizer(speech_recognizer)
        phrase_list.addPhrase("Jet Bot")

        print(f"Transcribing file: {file_path}")

        # Create a list to store the transcriptions
        transcription = []

        # Event handler for recognized speech
        def recognized_handler(evt):
            print(f"Recognized: {evt.result.text}")
            transcription.append(evt.result.text)

        # Connect the recognized event to the handler
        speech_recognizer.recognized.connect(recognized_handler)

        # Start continuous recognition
        speech_recognizer.start_continuous_recognition()

        # Wait for the transcription to complete
        print("Transcribing... Press Ctrl+C to stop manually if needed.")
        while True:
            pass  # Wait indefinitely or until manual interruption (Ctrl+C)

        # Full transcription output
        full_transcription = ' '.join(transcription)
        print(f"Full Transcript: {full_transcription}")

    except Exception as e:
        print(f"An error occurred: {e}")
```

### Explanation:
- **Phrase List**: The `PhraseListGrammar` object allows you to specify words or phrases that should be recognized more reliably. In this case, adding `"Jet Bot"` ensures that this phrase gets recognized properly, even if the default model would otherwise misinterpret it.
- You can add more phrases as needed to improve recognition accuracy for specific terms.

### 3. **Improve Audio Quality**
Improving the quality of the audio files (if possible) can also make a significant difference in transcription accuracy:
- **Reduce background noise**: Noise-cancellation techniques or better recording conditions can reduce the chance of misinterpretations.
- **Normalize audio levels**: Ensuring consistent volume levels helps the model to process the audio more effectively.

### 4. **Language and Locale Tuning**
Ensure that the correct **language locale** is being set in the Speech SDK. If the audio contains regional accents, try setting a more specific locale to improve recognition.

For example, if the speech is in American English, set it explicitly like this:

```python
speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region, speech_recognition_language="en-US")
```

### 5. **Post-processing of Transcriptions**
As a fallback, you could perform **post-processing** on the transcriptions to correct common misinterpretations. For example, if you consistently see "JetBlue" being transcribed instead of "Jet Bot", you can use simple string replacement after transcription:

```python
full_transcription = full_transcription.replace("JetBlue", "Jet Bot")
```

This is not ideal, but it can be a quick fix for terms that are consistently misrecognized.

### Conclusion:
- **Phrase lists** are a quick and effective way to improve the accuracy of specific terms.
- **Custom Speech** can provide more tailored results if you frequently encounter specific terminology.
- **Improving audio quality** or using the correct **language locale** can also help in general transcription accuracy.

Let me know how this works for you or if you need further clarification!

Improving the audio quality is crucial for ensuring more accurate transcriptions, especially in automated systems like speech-to-text services. Below are several ways to improve audio quality at various stages, including **during recording**, **post-processing**, and **using software tools**.

### 1. **Improve Audio Quality During Recording**
If you have control over the audio recording environment, here are some best practices to capture high-quality audio:

#### a. **Reduce Background Noise**
- **Quiet environment**: Ensure that recordings are done in a quiet environment. Close windows, doors, and turn off noisy devices.
- **Use noise-cancelling microphones**: High-quality microphones with built-in noise cancellation can significantly reduce background noise during recording.
- **Acoustic treatment**: Soft materials like carpets, curtains, or foam panels in the room can absorb echo and reduce noise.

#### b. **Use a High-Quality Microphone**
- **Directional microphones**: These are better at picking up sound from one direction (the speaker) while ignoring background noise.
- **External microphones**: Avoid built-in microphones on laptops or phones if possible, as external microphones provide much better clarity.

#### c. **Microphone Positioning**
- Keep the microphone at a proper distance from the speaker (6–12 inches is a good starting point).
- Avoid plosive sounds (e.g., "P", "T") by angling the microphone slightly off-center from the speaker’s mouth.

#### d. **Record in High-Quality Formats**
- **Use a higher bitrate**: If you are recording audio, try to record in a format like WAV (uncompressed) with a sample rate of 16,000 Hz or higher, and 16-bit PCM encoding. Avoid lossy formats like MP3 if possible during recording.
- **Avoid over-compression**: If you must use MP3 or another compressed format, keep the bitrate at 128 kbps or higher.

---

### 2. **Post-Processing Audio Files**
If you’re working with existing audio files, post-processing can improve clarity, reduce noise, and balance levels.

#### a. **Use Audio Editing Software**
You can use audio editing software like **Audacity** or **Adobe Audition** to clean up the audio. Here's how:

##### i. **Noise Reduction (Audacity Example)**
- Open your audio file in **Audacity**.
- Select a segment of the audio that contains only background noise.
- Go to `Effect > Noise Reduction`, and click `Get Noise Profile`.
- Select the entire audio file, return to `Effect > Noise Reduction`, and apply the noise reduction.

##### ii. **Equalization**
- Equalization can help improve the clarity of speech by boosting mid-range frequencies (around 1–5 kHz) where human speech typically falls.
- In **Audacity**: Go to `Effect > Equalization` and adjust the frequencies to boost clarity.

##### iii. **Compression**
- Dynamic range compression can help balance the loudness of your recording, making quieter parts louder and reducing overly loud parts. This ensures more consistent volume levels, which is important for automated speech recognition.
- In **Audacity**: Go to `Effect > Compressor` and adjust the settings based on your needs.

##### iv. **Normalize Audio**
- Normalizing the audio adjusts the volume of your recording so that it is consistent throughout.
- In **Audacity**: Go to `Effect > Normalize`.

---

### 3. **Automated Audio Processing Tools**
If you want a more automated approach, there are tools and APIs that can clean up audio files with minimal manual intervention.

#### a. **Auphonic**
[Auphonic](https://auphonic.com/) is an automated audio post-production service that can improve audio quality without requiring extensive manual edits. It offers:
- **Noise reduction**: Automatically removes background noise.
- **Loudness normalization**: Adjusts the audio to consistent loudness levels.
- **Multitrack mixing**: Automatically balances audio tracks.

#### b. **Krisp (for Live Calls)**
[Krisp](https://krisp.ai/) is a noise-cancelling app designed for live calls and recordings. It can:
- Remove background noise in real-time while recording or during live calls.
- Work with any recording app or conferencing software.

#### c. **Izotope RX (Advanced Audio Restoration)**
[Izotope RX](https://www.izotope.com/en/products/rx.html) is a professional audio restoration tool used for cleaning up audio, especially in challenging conditions. It can:
- **Remove clicks, pops, and hum**.
- **De-reverb**: Reduce echo in recordings.
- **De-noise**: Advanced noise reduction that can clean up complex audio issues.

---

### 4. **Programmatic Audio Processing in Python**
If you're processing audio programmatically, you can use Python libraries like `pydub`, `librosa`, and `scipy` to clean up audio automatically.

#### a. **Noise Reduction with Python**
You can use libraries like `noisereduce` to reduce noise in audio files programmatically.

```bash
pip install noisereduce librosa numpy
```

```python
import noisereduce as nr
import librosa
import soundfile as sf

# Load the audio file
audio, rate = librosa.load('input_audio.wav', sr=None)

# Apply noise reduction
reduced_noise = nr.reduce_noise(y=audio, sr=rate)

# Save the cleaned audio
sf.write('output_audio.wav', reduced_noise, rate)
```

This script will reduce noise in your audio file and output a cleaner version.

---

### 5. **Optimize Audio for Speech Recognition**
To get the best results with Azure Speech or other speech-to-text services, ensure that the audio is optimized for speech.

#### a. **Sample Rate**
- Azure Speech SDK works best with audio that is recorded or resampled to **16 kHz**. If the audio is at a different sample rate, consider converting it.

#### b. **Mono vs Stereo**
- Convert stereo audio to mono if possible. Many speech-to-text models work best with mono audio because it's simpler to process.

You can use `pydub` to convert an audio file to the optimal format:

```python
from pydub import AudioSegment

# Load stereo audio
audio = AudioSegment.from_file("stereo_audio.mp3")

# Convert to mono and resample to 16kHz
audio = audio.set_channels(1).set_frame_rate(16000)

# Export as WAV
audio.export("output_audio.wav", format="wav")
```

---

### 6. **Use Azure’s Built-in Features**
Azure Speech service has some built-in features that help improve audio quality during transcription.

#### a. **Enable Automatic Noise Suppression**
In Azure Speech, you can enable built-in noise suppression to improve the transcription of noisy audio.

```python
speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)
speech_config.enable_audio_logging = True
```

---

### Summary of Steps:
1. **Recording best practices**: Use good microphones, quiet environments, and appropriate recording techniques.
2. **Post-process audio**: Use tools like Audacity, Auphonic, or Python libraries for noise reduction, equalization, and normalization.
3. **Use automated tools**: Consider using services like Auphonic or Izotope RX for advanced or automated cleaning.
4. **Optimize for speech recognition**: Ensure proper sample rate (16 kHz) and mono channel for transcription.
5. **Leverage Azure’s noise suppression**: Enable built-in features in Azure for better handling of noise.

Let me know if you need more help with any of these steps!
