# **Web Scraping Project Checklist**

## **ðŸ”§ 1. Initial Setup**

âœ… **Set up a virtual environment**

```powershell
python -m venv venv
.\venv\Scripts\activate  # Windows
source venv/bin/activate  # Mac/Linux
```

âœ… **Install required dependencies**

```powershell
pip install requests beautifulsoup4 pandas tqdm lxml
pip install black flake8 pytest pre-commit
pip freeze > requirements.txt
```

âœ… **Set up version control (Git)**

```powershell
git init
git add .
git commit -m "Initial project setup"
git branch -M main
```

âœ… **Set up `.gitignore`**  
Add exclusions for `venv/`, `__pycache__/`, `.env`, and data files.

âœ… **Create project structure**

```
webscraper_project/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â”œâ”€â”€ jets_pizza_scraper.py
â”‚   â”œâ”€â”€ processors/
â”‚   â”‚   â”œâ”€â”€ data_processor.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ logger.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_scraper.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
```

---

## **ðŸ“¡ 2. Scraper Development**

âœ… **Inspect website structure**

- **Visit** `https://www.jetspizza.com/stores/`
- **Identify state links** (`a.pge-find-store__state-item`)
- **Check store addresses** (`div.pge-find-store__item > p:nth-child(2)`)
- **Ensure compliance with `robots.txt`**

âœ… **Write the scraper to get state URLs**

- Use `requests` to fetch the HTML
- Parse the HTML with `BeautifulSoup`
- Extract all state links

âœ… **Write the scraper to get store addresses**

- Loop through each stateâ€™s store page
- Extract store addresses
- Handle missing or malformed data

âœ… **Implement rate limiting**

- **Add a delay between requests** (`time.sleep(1)`)
- **Use session management** (`requests.Session()`)

âœ… **Handle errors and retries**

```python
def retry_request(url, retries=3, delay=5):
    for attempt in range(retries):
        response = requests.get(url)
        if response.status_code == 200:
            return response
        time.sleep(delay)
    return None
```

âœ… **Save extracted data to a CSV file**

```python
import csv

def save_to_csv(data, filename="jets_pizza_stores.csv"):
    with open(filename, "w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["State", "Store Address"])
        writer.writerows(data)
```

---

## **ðŸ›  3. Data Processing**

âœ… **Clean and transform extracted data**

- **Remove duplicates**
- **Standardize addresses**

âœ… **Validate extracted data**

- Check for missing store addresses
- Ensure correct formatting

âœ… **Save cleaned data to a new CSV file**

```python
import pandas as pd

df = pd.read_csv("jets_pizza_stores.csv")
df.drop_duplicates(inplace=True)
df.to_csv("cleaned_stores.csv", index=False)
```

---

## **ðŸ” 4. Testing & Debugging**

âœ… **Write unit tests for scraper functions**

```python
def test_fetch_states():
    scraper = JetsPizzaScraper()
    scraper.fetch_states()
    assert len(scraper.state_urls) > 0
```

âœ… **Test data extraction accuracy**

- Print sample extracted data
- Compare against actual website

âœ… **Run test suite**

```powershell
pytest tests/
```

âœ… **Fix any issues in scraping logic**

---

## **ðŸ“Š 5. Logging & Monitoring**

âœ… **Set up logging for debugging**

```python
import logging

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
```

âœ… **Monitor request failures**

- Log error messages when requests fail
- Track request success rate

âœ… **Alert on excessive errors**

- Send notifications if failure rate > 10%

---

## **ðŸš€ 6. Deployment & Execution**

âœ… **Run the scraper**

```powershell
python src/scrapers/jets_pizza_scraper.py
```

âœ… **Verify the generated CSV file**

- Ensure all states are included
- Check store addresses for accuracy

âœ… **Schedule periodic scraping (Optional)**

```powershell
# Run scraper every day at midnight
echo "0 0 * * * python src/scrapers/jets_pizza_scraper.py" | crontab -
```

âœ… **Backup extracted data**

- Save CSV files to cloud storage

---

## **ðŸ“ˆ 7. Future Enhancements**

âœ… **Improve scraper efficiency**

- Implement multithreading for faster requests
- Store data in a database instead of CSV

âœ… **Add proxy support**

```python
proxies = {
    "http": "http://yourproxy.com",
    "https": "https://yourproxy.com",
}
requests.get(url, proxies=proxies)
```

âœ… **Develop an API to serve store data**

- Create a Flask/Django API to access the scraped data

---

### **ðŸŽ¯ Final Steps:**

âœ… **Check logs for errors**  
âœ… **Ensure CSV file is complete**  
âœ… **Review scraped data quality**  
âœ… **Store backups of extracted data**  
âœ… **Plan for future optimizations**

ðŸš€ **Project Complete! ðŸŽ‰**